% Encoding: UTF-8

@Article{Engelhardt2017,
  author    = {Engelhardt, Benjamin and Kschischo, Maik and Fröhlich, Holger},
  title     = {{A Bayesian approach to estimating hidden variables as well as missing and wrong molecular interactions in ordinary differential equation-based mathematical models}},
  doi       = {10.1098/rsif.2017.0332},
  issn      = {1742-5689},
  number    = {131},
  pages     = {20170332},
  volume    = {14},
  abstract  = {Ordinary differential equations (ODEs) are a popular approach to quantitatively model molecular networks based on biological knowledge. However, such knowledge is typically restricted. Wrongly modelled biological mechanisms as well as relevant external influence factors that are not included into the model are likely to manifest in major discrepancies between model predictions and experimental data. Finding the exact reasons for such observed discrepancies can be quite challenging in practice. In order to address this issue, we suggest a Bayesian approach to estimate hidden influences in ODE-based models. The method can distinguish between exogenous and endogenous hidden influences. Thus, we can detect wrongly specified as well as missed molecular interactions in the model. We demonstrate the performance of our Bayesian dynamic elastic-net with several ordinary differential equation models from the literature, such as human JAK–STAT signalling, information processing at the erythropoietin receptor, isomerization of liquid $\alpha$ -Pinene, G protein cycling in yeast and UV-B triggered signalling in plants. Moreover, we investigate a set of commonly known network motifs and a gene-regulatory network. Altogether our method supports the modeller in an algorithmic manner to identify possible sources of errors in ODE-based models on the basis of experimental data.},
  file      = {:files/Engelhardt2017.pdf:pdf},
  journal   = {Journal of The Royal Society Interface},
  keywords  = {Dynamic elastic-net, Modelling, Ordinary differential equations, Systems biology},
  month     = {jun},
  pmid      = {28615495},
  publisher = {Royal Society Publishing},
  year      = {2017},
}

@Article{Lynch2019,
  author    = {Lynch, Scott M. and Bartlett, Bryce},
  title     = {{Bayesian Statistics in Sociology: Past, Present, and Future}},
  doi       = {10.1146/annurev-soc-073018-022457},
  issn      = {0360-0572},
  number    = {1},
  pages     = {47--68},
  volume    = {45},
  abstract  = {Although Bayes' theorem has been around for more than 250 years, widespread application of the Bayesian approach only began in statistics in 1990. By 2000, Bayesian statistics had made considerable headway into social science, but even now its direct use is rare in articles in top sociology journals, perhaps because of a lack of knowledge about the topic. In this review, we provide an overview of the key ideas and terminology of Bayesian statistics, and we discuss articles in the top journals that have used or developed Bayesian methods over the last decade. In this process, we elucidate some of the advantages of the Bayesian approach. We highlight that many sociologists are, in fact, using Bayesian methods, even if they do not realize it, because techniques deployed by popular software packages often involve Bayesian logic and/or computation. Finally, we conclude by briefly discussing the future of Bayesian statistics in sociology.},
  journal   = {Annual Review of Sociology},
  keywords  = {Bayesian statistics, Markov chain Monte Carlo, crisis in science, missing data, model selection, multiple imputation},
  month     = {jul},
  publisher = {Annual Reviews Inc.},
  year      = {2019},
}

@Article{Jayawardhana2008,
  author   = {Jayawardhana, Bayu and Kell, Douglas B. and Rattray, Magnus},
  title    = {{Bayesian inference of the sites of perturbations in metabolic pathways via Markov chain Monte Carlo}},
  doi      = {10.1093/bioinformatics/btn103},
  issn     = {1367-4803},
  number   = {9},
  pages    = {1191--1197},
  volume   = {24},
  abstract = {Motivation: Genetic modifications or pharmaceutical interventions can influence multiple sites in metabolic pathways, and often these are 'distant' from the primary effect. In this regard, the ability to identify target and off-target effects of a specific compound or gene therapy is both a major challenge and critical in drug discovery. Results: We applied Markov Chain Monte Carlo (MCMC) for parameter estimation and perturbation identification in the kinetic modeling of metabolic pathways. Variability in the steady-state measurements in cells taken from a population can be caused by differences in initial conditions within the population, by variation of parameters among individuals and by possible measurement noise. MCMC-based parameter estimation is proposed as a method to help in inferring parameter distributions, taking into account uncertainties in the initial conditions and in the measurement data. The inferred parameter distributions are then used to predict changes in the network via a simple classification method. The proposed technique is applied to analyze changes in the pathways of pyruvate metabolism of mutants of Lactococcus lactis, based on previously published experimental data. {\textcopyright} The Author 2008. Published by Oxford University Press. All rights reserved.},
  file     = {:files/Jayawardhana2008.pdf:pdf},
  journal  = {Bioinformatics},
  pmid     = {18356193},
  year     = {2008},
}

@Article{Ashkenasy2017,
  author   = {Ashkenasy, Gonen and Hermans, Thomas M and Otto, Sijbren and Taylor, Annette F},
  title    = {{Systems chemistry}},
  doi      = {10.1039/C7CS00117G},
  issn     = {0306-0012},
  number   = {9},
  pages    = {2543--2554},
  url      = {http://xlink.rsc.org/?DOI=C7CS00117G},
  volume   = {46},
  abstract = {A series of exciting phenomena that can occur in supramolecular systems away from equilibrium are reviewed.},
  file     = {:files/Ashkenasy2017.pdf:pdf},
  journal  = {Chemical Society Reviews},
  year     = {2017},
}

@Article{Vehtari2017,
  author        = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  title         = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
  doi           = {10.1007/s11222-016-9696-4},
  eprint        = {1507.04544},
  issn          = {0960-3174},
  number        = {5},
  pages         = {1413--1432},
  volume        = {27},
  abstract      = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  archiveprefix = {arXiv},
  arxivid       = {1507.04544},
  file          = {:files/Vehtari2017.pdf:pdf},
  journal       = {Statistics and Computing},
  keywords      = {Bayesian computation, K-fold cross-validation, Leave-one-out cross-validation (LOO), Pareto smoothed importance sampling (PSIS), Stan, Widely applicable information criterion (WAIC)},
  month         = {sep},
  publisher     = {Springer Science and Business Media, LLC},
  year          = {2017},
}

@Article{Wong2017a,
  author   = {Wong, Albert S.Y. Y and Huck, Wilhelm T.S. S},
  title    = {{Grip on complexity in chemical reaction networks}},
  doi      = {10.3762/bjoc.13.147},
  issn     = {1860-5397},
  pages    = {1486--1497},
  url      = {https://www.beilstein-journals.org/bjoc/articles/13/147},
  volume   = {13},
  abstract = {A new discipline of “systems chemistry” is emerging, which aims to capture the complexity observed in natural systems within a synthetic chemical framework. Living systems rely on complex networks of chemical reactions to control the concentration of molecules in space and time. Despite the enormous complexity in biological networks, it is possible to identify network motifs that lead to functional outputs such as bistability or oscillations. To truly understand how living systems function, we need a complete understanding of how chemical reaction networks (CRNs) create function. We propose the development of a bottom-up approach to design and construct CRNs where we can follow the influence of single chemical entities on the properties of the network as a whole. Ultimately, this approach should allow us to not only understand such complex networks but also to guide and control their behavior.},
  file     = {:files/Wong2017a.pdf:pdf},
  journal  = {Beilstein Journal of Organic Chemistry},
  keywords = {Chemical reaction network, Complexity, Dissipative systems, Network motifs, Out-of-equilibrium, Tunability},
  month    = {jul},
  year     = {2017},
}

@Article{Barabasi2004,
  author   = {Barabási, Albert-László and Oltvai, Zoltán N.},
  title    = {{Network biology: understanding the cell's functional organization}},
  doi      = {10.1038/nrg1272},
  issn     = {1471-0056},
  number   = {2},
  pages    = {101--113},
  url      = {www.nature.com/reviews/genetics http://www.nature.com/articles/nrg1272 www.nature.com/reviews/genetics http://www.nature.com/articles/nrg1272 c:{\%}5CUsers{\%}5CMathieu{\%}5CGoogle Drive (m.g.baltussen@gmail.com){\%}5Creferences{\%}5Cfiles{\%}5CBarabasi2004 - Network Biolo},
  volume   = {5},
  abstract = {Reductionism, which has dominated biological research for over a century, has provided a wealth of knowledge about individual cellular components and their functions. Despite its enormous success, it is increasingly clear that a discrete biological function can only rarely be attributed to an individual molecule. Instead, most biological characteristics arise from complex interactions between the cell's numerous constituents, such as proteins, DNA, RNA and small molecules 1-8. Therefore, a key challenge for biology in the twenty-first century is to understand the structure and the dynamics of the complex intercellular web of interactions that contribute to the structure and function of a living cell. The development of high-throughput data-collection techniques, as epitomized by the widespread use of microarrays, allows for the simultaneous interrogation of the status of a cell's components at any given time. In turn, new technology platforms, such as PROTEIN CHIPS or semi-automated YEAST TWO-HYBRID SCREENS, help to determine how and when these molecules interact with each other. Various types of interaction webs, or networks, (including protein-protein interaction, metabolic, signalling and transcription-regulatory networks) emerge from the sum of these interactions. None of these networks are independent, instead they form a 'network of networks' that is responsible for the behaviour of the cell. A major challenge of contemporary biology is to embark on an integrated theoretical and experimental programme to map out, understand and model in quan-tifiable terms the topological and dynamic properties of the various networks that control the behaviour of the cell. Help along the way is provided by the rapidly developing theory of complex networks that, in the past few years, has made advances towards uncovering the organizing principles that govern the formation and evolution of various complex technological and social networks 9-12. This research is already making an impact on cell biology. It has led to the realization that the architectural features of molecular interaction networks within a cell are shared to a large degree by other complex systems, such as the Internet, computer chips and society. This unexpected universality indicates that similar laws may govern most complex networks in nature, which allows the expertise from large and well-mapped non-biological systems to be used to characterize the intricate interwoven relationships that govern cellular functions. In this review, we show that the quantifiable tools of network theory offer unforeseen possibilities to understand the cell's internal organization and evolution, fundamentally altering our view of cell biology. The emerging results are forcing the realization that, notwithstanding the importance of individual molecules, cellular function is a contextual attribute of strict and quantifiable patterns of interactions between the myriad of cellular constituents. Although uncovering the generic organizing principles of cellular networks A key aim of postgenomic biomedical research is to systematically catalogue all molecules and their interactions within a living cell. There is a clear need to understand how these molecules and the interactions between them determine the function of this enormously complex machinery, both in isolation and when surrounded by other cells. Rapid advances in network biology indicate that cellular networks are governed by universal laws and offer a new conceptual framework that could potentially revolutionize our view of biology and disease pathologies in the twenty-first century. PROTEIN CHIPS Similar to cDNA microarrays, this evolving technology involves arraying a genomic set of proteins on a solid surface without denaturing them. The proteins are arrayed at a high enough density for the detection of activity, binding to lipids and so on.},
  file     = {:files/Barabasi2004.pdf:pdf},
  journal  = {Nature Reviews Genetics},
  month    = {feb},
  year     = {2004},
}

@Article{Purnick2009,
  author    = {Purnick, Priscilla E.M. and Weiss, Ron},
  title     = {{The second wave of synthetic biology: From modules to systems}},
  doi       = {10.1038/nrm2698},
  issn      = {1471-0072},
  number    = {6},
  pages     = {410--422},
  volume    = {10},
  abstract  = {Synthetic biology is a research field that combines the investigative nature of biology with the constructive nature of engineering. Efforts in synthetic biology have largely focused on the creation and perfection of genetic devices and small modules that are constructed from these devices. But to view cells as true 'programmable' entities, it is now essential to develop effective strategies for assembling devices and modules into intricate, customizable larger scale systems. The ability to create such systems will result in innovative approaches to a wide range of applications, such as bioremediation, sustainable energy production and biomedical therapies. {\textcopyright} 2009 Macmillan Publishers Limited. All rights reserved.},
  file      = {:Purnick2009.pdf:PDF},
  journal   = {Nature Reviews Molecular Cell Biology},
  pmid      = {19461664},
  publisher = {Nature Publishing Group},
  year      = {2009},
}

@Article{Lillacci2010,
  author   = {Lillacci, G and Khammash, M},
  title    = {{Parameter Estimation and Model Selection in Computational Biology}},
  doi      = {10.1371/journal.pcbi.1000696},
  number   = {3},
  pages    = {1000696},
  url      = {www.ploscompbiol.org},
  volume   = {6},
  abstract = {A central challenge in computational modeling of biological systems is the determination of the model parameters. Typically, only a fraction of the parameters (such as kinetic rate constants) are experimentally measured, while the rest are often fitted. The fitting process is usually based on experimental time course measurements of observables, which are used to assign parameter values that minimize some measure of the error between these measurements and the corresponding model prediction. The measurements, which can come from immunoblotting assays, fluorescent markers, etc., tend to be very noisy and taken at a limited number of time points. In this work we present a new approach to the problem of parameter selection of biological models. We show how one can use a dynamic recursive estimator, known as extended Kalman filter, to arrive at estimates of the model parameters. The proposed method follows. First, we use a variation of the Kalman filter that is particularly well suited to biological applications to obtain a first guess for the unknown parameters. Secondly, we employ an a posteriori identifiability test to check the reliability of the estimates. Finally, we solve an optimization problem to refine the first guess in case it should not be accurate enough. The final estimates are guaranteed to be statistically consistent with the measurements. Furthermore, we show how the same tools can be used to discriminate among alternate models of the same biological process. We demonstrate these ideas by applying our methods to two examples, namely a model of the heat shock response in E. coli, and a model of a synthetic gene regulation system. The methods presented are quite general and may be applied to a wide class of biological systems where noisy measurements are used for parameter estimation or model selection.},
  file     = {:files/Lillacci2010.pdf:pdf},
  journal  = {PLoS Comput Biol},
  year     = {2010},
}

@Article{Salvatier2016,
  author    = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
  title     = {Probabilistic programming in {Python} using {PyMC3}},
  doi       = {10.7717/peerj-cs.55},
  issn      = {2376-5992},
  language  = {en},
  pages     = {e55},
  url       = {https://peerj.com/articles/cs-55},
  urldate   = {2021-12-13},
  volume    = {2},
  abstract  = {Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.},
  file      = {Full Text PDF:https\://peerj.com/articles/cs-55.pdf:application/pdf},
  journal   = {PeerJ Computer Science},
  month     = apr,
  publisher = {PeerJ Inc.},
  year      = {2016},
}

@Unpublished{FernandezRegueiro2021,
  author = {Fernández Regueiro, Cris Lia and Ivanov, Nikita and Van de Wiel, Jeroen and Baltussen, Mathieu G. and Huck, Wilhelm T. S.},
  title  = {Performing Arithmetic Operations using Enzymatic Reaction Networks},
  year   = {2021},
}

@phdthesis{Rivello2020,
  title={Droplet Microfluidics for single-cell analysis},
  author={Rivello, Francesca M.},
  year={2020},
  school={[Sl: sn]}
}

@Article{Toussaint2011,
  author   = {von Toussaint, Udo},
  title    = {{Bayesian inference in physics}},
  doi      = {10.1103/RevModPhys.83.943},
  issn     = {0034-6861},
  number   = {3},
  pages    = {943--999},
  volume   = {83},
  abstract = {Bayesian inference provides a consistent method for the extraction of information from physics experiments even in ill-conditioned circumstances. The approach provides a unified rationale for data analysis, which both justifies many of the commonly used analysis procedures and reveals some of the implicit underlying assumptions. This review summarizes the general ideas of the Bayesian probability theory with emphasis on the application to the evaluation of experimental data. As case studies for Bayesian parameter estimation techniques examples ranging from extra-solar planet detection to the deconvolution of the apparatus functions for improving the energy resolution and change point estimation in time series are discussed. Special attention is paid to the numerical techniques suited for Bayesian analysis, with a focus on recent developments of Markov chain Monte Carlo algorithms for high-dimensional integration problems. Bayesian model comparison, the quantitative ranking of models for the explanation of a given data set, is illustrated with examples collected from cosmology, mass spectroscopy, and surface physics, covering problems such as background subtraction and automated outlier detection. Additionally the Bayesian inference techniques for the design and optimization of future experiments are introduced. Experiments, instead of being merely passive recording devices, can now be designed to adapt to measured data and to change the measurement strategy on the fly to maximize the information of an experiment. The applied key concepts and necessary numerical tools which provide the means of designing such inference chains and the crucial aspects of data fusion are summarized and some of the expected implications are highlighted. {\textcopyright} 2011 American Physical Society.},
  file     = {:files/Toussaint2011.pdf:pdf},
  journal  = {Reviews of Modern Physics},
  keywords = {0705Fb, 8280{\`{A}}d, 8920Ff CONTENTS, numbers: 0250Tt},
  month    = {sep},
  year     = {2011},
}

@Article{Huang2020,
  author   = {Huang, Yuanzhi and Gilmour, Steven G and Mylona, Kalliopi and Goos, Peter},
  title    = {{Optimal Design of Experiments for Hybrid Nonlinear Models, with Applications to Extended Michaelis-Menten Kinetics}},
  doi      = {10.1007/s13253-020-00405-3},
  abstract = {Biochemical mechanism studies often assume statistical models derived from Michaelis-Menten kinetics, which are used to approximate initial reaction rate data given the concentration level of a single substrate. In experiments dealing with industrial applications, however, there are typically a wide range of kinetic profiles where more than one factor is controlled. We focus on optimal design of such experiments requiring the use of multifactor hybrid nonlinear models, which presents a considerable computational challenge. We examine three different candidate models and search for tailor-made D-or weighted-A-optimal designs that can ensure the efficiency of nonlinear least squares estimation. We also study a compound design criterion for discriminating between two candidate models, which we recommend for design of advanced kinetic studies. Supplementary materials accompanying this paper appear on-line},
  file     = {:files/Huang2020.pdf:pdf},
  keywords = {Biochemistry, Compound criterion, D-optimality, Exchange algorithm, Weighted-A-optimality},
  year     = {2020},
}

@Article{Gabor2017,
  author    = {Gábor, Attila and Villaverde, Alejandro F. and Banga, Julio R.},
  title     = {{Parameter identifiability analysis and visualization in large-scale kinetic models of biosystems}},
  doi       = {10.1186/s12918-017-0428-y},
  issn      = {1752-0509},
  number    = {1},
  pages     = {54},
  volume    = {11},
  abstract  = {Background: Kinetic models of biochemical systems usually consist of ordinary differential equations that have many unknown parameters. Some of these parameters are often practically unidentifiable, that is, their values cannot be uniquely determined from the available data. Possible causes are lack of influence on the measured outputs, interdependence among parameters, and poor data quality. Uncorrelated parameters can be seen as the key tuning knobs of a predictive model. Therefore, before attempting to perform parameter estimation (model calibration) it is important to characterize the subset(s) of identifiable parameters and their interplay. Once this is achieved, it is still necessary to perform parameter estimation, which poses additional challenges. Methods: We present a methodology that (i) detects high-order relationships among parameters, and (ii) visualizes the results to facilitate further analysis. We use a collinearity index to quantify the correlation between parameters in a group in a computationally efficient way. Then we apply integer optimization to find the largest groups of uncorrelated parameters. We also use the collinearity index to identify small groups of highly correlated parameters. The results files can be visualized using Cytoscape, showing the identifiable and non-identifiable groups of parameters together with the model structure in the same graph. Results: Our contributions alleviate the difficulties that appear at different stages of the identifiability analysis and parameter estimation process. We show how to combine global optimization and regularization techniques for calibrating medium and large scale biological models with moderate computation times. Then we evaluate the practical identifiability of the estimated parameters using the proposed methodology. The identifiability analysis techniques are implemented as a MATLAB toolbox called VisId, which is freely available as open source from GitHub ( https://github.com/gabora/visid ). Conclusions: Our approach is geared towards scalability. It enables the practical identifiability analysis of dynamic models of large size, and accelerates their calibration. The visualization tool allows modellers to detect parts that are problematic and need refinement or reformulation, and provides experimentalists with information that can be helpful in the design of new experiments.},
  file      = {:files/2017_Gábor, Villaverde, Banga_Parameter identifiability analysis and visualization in large-scale kinetic models of biosystems.pdf:pdf},
  journal   = {BMC Systems Biology},
  keywords  = {Dynamic models, Global optimization, Identifiability, Overfitting, Parameter estimation, Regularization},
  month     = {may},
  pmid      = {28476119},
  publisher = {BioMed Central Ltd.},
  year      = {2017},
}

@Article{Kholodenko2006,
  author   = {Kholodenko, Boris N},
  title    = {{Cell-signalling dynamics in time and space}},
  doi      = {10.1038/nrm1838},
  issn     = {1471-0072},
  number   = {3},
  pages    = {165--176},
  url      = {www.nature.com/reviews/molcellbio http://www.nature.com/articles/nrm1838},
  volume   = {7},
  abstract = {Apoptosis Active cell death; the process by which cells commit suicide. Steady state A dynamic system state that does not change over time. If a system is described by differential equations, a steady state is determined by equating the time derivatives of all variables to zero. Brownian motion Random, thermal motion of molecular species. Abstract | The specificity of cellular responses to receptor stimulation is encoded by the spatial and temporal dynamics of downstream signalling networks. Temporal dynamics are coupled to spatial gradients of signalling activities, which guide pivotal intracellular processes and tightly regulate signal propagation across a cell. Computational models provide insights into the complex relationships between the stimuli and the cellular responses, and reveal the mechanisms that are responsible for signal amplification, noise reduction and generation of discontinuous bistable dynamics or oscillations. Cells respond to external cues using a limited number of signalling pathways that are activated by plasma-membrane receptors, such as G protein-coupled receptors (GPCRs) and receptor tyrosine kinases (RTKs). These pathways do not simply transmit, but they also process, encode and integrate internal and external signals. Recently, it has become apparent that distinct spatio-temporal activation profiles of the same repertoire of signalling proteins result in different gene-expression patterns and diverse physiological responses 1-3. These observations indicate that pivotal cellular decisions, such as cytoskeletal reorganization, cell-cycle checkpoints and cell death (apoptosis), depend on the precise temporal control and relative spatial distribution of activated signal transducers. RTK-mediated signalling pathways have been in the limelight of scientific interest owing to their central role in the regulation of embryogenesis, cell survival, motility, proliferation, differentiation, glucose metabolism and apoptosis 4-6. Malfunction of RTK signalling is a leading cause of important human diseases that range from developmental defects to cancer, chronic inflammatory syndromes and diabetes 6-8. Upon stimulation, RTKs undergo dimerization (for example , the epidermal growth factor receptor (EGFR)) or allosteric transitions (insulin receptor) that result in the activation of the intrinsic tyrosine-kinase activity 4,9. Subsequent phosphorylation of multiple tyrosine residues on the receptor transmits a biochemical signal to numerous cytoplasmic proteins, thereby triggering their mobilization to the cell surface 4,10. The resulting cellular responses occur through complex biochemical circuits of protein-protein interactions and covalent-modification cascades. An emerging picture of interconnected signalling networks has replaced the earlier concepts of discrete linear pathways, which relate extracellular signals to the expression of specific genes, raising questions about the specificity of signal-response events. In fact, the protein complement that mediates signal transduction downstream of RTKs is similar for all the RTK-mediated pathways 11. Both GPCRs and RTKs activate kinase and phosphatase cascades, such as mitogen-activated protein kinase (MAPK) cascades, that induce the expression of nuclear transcription factors. For any individual receptor pathway, there is no single protein or gene that is responsible for signalling specificity. Instead, specificity is determined by the temporal and spatial dynamics of downstream signalling components. A classical example is the distinct biological outcome of the PC12 cell-line stimulation with EGF and nerve growth factor (NGF). EGF induces transient MAPK activation, which results in cell proliferation, whereas a sustained MAPK activation by NGF changes the cell fate and induces cell differentiation 1,2. However, the factors that control the kinetics of MAPK cascades are intricate. MAPK cascades can generate bistable dynamics (in which two stable 'on' and 'off ' steady states coexist), abrupt switches and oscillations 12-14 , and their responses depend dramatically on their subcellular localization and their recruitment to scaffold proteins 15,16. The purpose of this review is to survey dynamic and spatial aspects of intracellular communication. Wherever possible, I outline the general principles by which chemical transformations and Brownian motion of myriad signalling molecules create coordinated behaviour in time and space and generate stimulus-specific responses. I also discuss how we can elucidate NATURE REVIEWS | MOLECULAR CELL BIOLOGY VOLUME 7 | MARCH 2006 | 165 REVIEWS},
  file     = {:files/Kholodenko2006.pdf:pdf},
  journal  = {Nature Reviews Molecular Cell Biology},
  month    = {mar},
  year     = {2006},
}

@Article{Maguire2020,
  author    = {Oliver R. Maguire and Albert S. Y. Wong and Mathieu G. Baltussen and Peer Duppen and Aleksandr A. Pogodaev and Wilhelm T. S. Huck},
  title     = {{Dynamic Environments as a Tool to Preserve Desired Output in a Chemical Reaction Network}},
  doi       = {10.1002/chem.201904725},
  issn      = {0947-6539},
  number    = {7},
  pages     = {1676--1682},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/chem.201904725 http://www.scopus.com/inward/record.url?eid=2-s2.0-85078671648{\&}partnerID=MN8TOARS},
  volume    = {26},
  abstract  = {Current efforts to design functional molecular systems have overlooked the importance of coupling out-of-equilibrium behaviour with changes in the environment. Here, the authors use an oscillating reaction network and demonstrate that the application of environmental forcing, in the form of periodic changes in temperature and in the inflow of the concentration of one of the network components, removes the dependency of the periodicity of this network on temperature or flow rates and enforces a stable periodicity across a wide range of conditions. Coupling a system to a dynamic environment can thus be used as a simple tool to regulate the output of a network. In addition, the authors show that coupling can also induce an increase in behavioural complexity to include quasi-periodic oscillations.},
  file      = {:files/Maguire2020.pdf:pdf},
  journal   = {Chemistry – A European Journal},
  keywords  = {chemical reaction networks, enzymes, kinetics, nonequilibrium processes, systems chemistry},
  month     = {feb},
  publisher = {Wiley},
  year      = {2020},
}

@Book{Silver2021,
  author    = {Silver, Nate},
  title     = {{The Signal and the Noise: Why so many predictions fail, but some don't}},
  isbn      = {978-1-59-420411-1},
  pages     = {534},
  publisher = {Penguin Group},
  year      = {2021},
}

@Article{St.John2019,
  author   = {{St. John}, Peter C. and Strutz, Jonathan and Broadbelt, Linda J and Tyo, Keith E. J. and Bomble, Yannick J},
  title    = {{Bayesian inference of metabolic kinetics from genome-scale multiomics data}},
  doi      = {10.1371/journal.pcbi.1007424},
  editor   = {Maranas, Costas D.},
  issn     = {1553-7358},
  number   = {11},
  pages    = {e1007424},
  volume   = {15},
  abstract = {Modern biological tools generate a wealth of data on metabolite and protein concentrations that can be used to help inform new strain designs. However, integrating these data sources to generate predictions of steady-state metabolism typically requires a kinetic description of the enzymatic reactions that occur within a cell. Parameterizing these kinetic models from biological data can be computationally difficult, especially as the amount of data increases. Robust methods must also be able to quantify the uncertainty in model parameters as a function of the available data, which can be particularly computationally intensive. The field of Bayesian inference offers a wide range of methods for estimating distributions in parameter uncertainty. However, these techniques are poorly suited to kinetic metabolic modeling due to the complex kinetic rate laws typically employed and the resulting dynamic system that must be solved. In this paper, we employ linear-logarithmic kinetics to simplify the calculation of steady-state flux distributions and enable efficient sampling and variational inference methods. We demonstrate that detailed information on the posterior distribution of kinetic model parameters can be obtained efficiently at a variety of different problem scales, including large-scale kinetic models trained on multiomics datasets. These results allow modern Bayesian machine learning tools to be leveraged in understanding biological data and developing new, efficient strain designs.},
  file     = {:files/St.John2019.pdf:pdf},
  journal  = {PLOS Computational Biology},
  month    = {nov},
  year     = {2019},
}

@Article{VivoTruyols2012,
  author    = {Vivó-Truyols, Gabriel},
  title     = {Bayesian {Approach} for {Peak} {Detection} in {Two}-{Dimensional} {Chromatography}},
  doi       = {10.1021/ac202124t},
  issn      = {0003-2700},
  number    = {6},
  pages     = {2622--2630},
  url       = {https://doi.org/10.1021/ac202124t},
  urldate   = {2021-12-13},
  volume    = {84},
  abstract  = {A new method for peak detection in two-dimensional chromatography is presented. In a first step, the method starts with a conventional one-dimensional peak detection algorithm to detect modulated peaks. In a second step, a sophisticated algorithm is constructed to decide which of the individual one-dimensional peaks have been originated from the same compound and should then be arranged in a two-dimensional peak. The merging algorithm is based on Bayesian inference. The user sets prior information about certain parameters (e.g., second-dimension retention time variability, first-dimension band broadening, chromatographic noise). On the basis of these priors, the algorithm calculates the probability of myriads of peak arrangements (i.e., ways of merging one-dimensional peaks), finding which of them holds the highest value. Uncertainty in each parameter can be accounted by adapting conveniently its probability distribution function, which in turn may change the final decision of the most probable peak arrangement. It has been demonstrated that the Bayesian approach presented in this paper follows the chromatographers’ intuition. The algorithm has been applied and tested with LC × LC and GC × GC data and takes around 1 min to process chromatograms with several thousands of peaks.},
  file      = {:VivoTruyols2012.pdf:PDF},
  journal   = {Analytical Chemistry},
  month     = mar,
  publisher = {American Chemical Society},
  year      = {2012},
}

@Article{Schoot2021,
  author    = {van de Schoot, Rens and Depaoli, Sarah and King, Ruth and Kramer, Bianca and Märtens, Kaspar and Tadesse, Mahlet G. and Vannucci, Marina and Gelman, Andrew and Veen, Duco and Willemsen, Joukje and Yau, Christopher},
  title     = {{Bayesian statistics and modelling}},
  doi       = {10.1038/s43586-020-00001-2},
  issn      = {2662-8449},
  number    = {1},
  pages     = {1},
  url       = {www.nature.com/nrmp http://www.nature.com/articles/s43586-020-00001-2},
  volume    = {1},
  abstract  = {Bayesian statistics is an approach to data analysis and parameter estimation based on Bayes' theorem. Unique for Bayesian statistics is that all observed and unob-served parameters in a statistical model are given a joint probability distribution, termed the prior and data distributions. The typical Bayesian workflow consists of three main steps (Fig. 1): capturing available knowledge about a given parameter in a statistical model via the prior distribution, which is typically determined before data collection; determining the likelihood function using the information about the parameters available in the observed data; and combining both the prior distribution and the likelihood function using Bayes' theorem in the form of the posterior distribution. The posterior distribution reflects one's updated knowledge, balancing prior knowledge with observed data, and is used to conduct inferences. Bayesian inferences are optimal when averaged over this joint probability distribution and inference for these quantities is based on their conditional distribution given the observed data. The basis of Bayesian statistics was first described in a 1763 essay written by Reverend Thomas Bayes and published by Richard Price 1 on inverse probability, or how to determine the probability of a future event solely based on past events. It was not until 1825 that Pierre Simon Laplace 2 published the theorem we now know as Bayes' theorem (Box 1). Although the ideas of inverse probability and Bayes' theorem are longstanding in mathematics, these tools became prominent in applied statistics in the past 50 years 3-10. We describe many advantages and disadvantages throughout the Primer. This Primer provides an overview of the current and future use of Bayesian statistics that is suitable for quantitative researchers working across a broad range of science-related areas that have at least some knowledge of regression modelling. We supply an overview of the literature that can be used for further study and illustrate how to implement a Bayesian model on real data. All of the data and code are available for teaching purposes. This Primer discusses the general framework of Bayesian statistics and introduces a Bayesian research cycle (Fig. 1). We first discuss formalizing of prior distributions , prior predictive checking and determining the likelihood distribution (Experimentation). We discuss relevant algorithms and model fitting, describe examples of variable selection and variational inference , and provide an example calculation with posterior predictive checking (Results). Then, we describe how Bayesian statistics are being used in different fields of science (Applications), followed by guidelines for data sharing, reproducibility and reporting standards (Reproducibility and data deposition). We conclude with a discussion on avoiding bias introduced by using incorrect models (Limitations and optimizations), and provide a look into the future with Bayesian artificial intelligence (Outlook). Abstract | Bayesian statistics is an approach to data analysis based on Bayes' theorem, where available knowledge about parameters in a statistical model is updated with the information in observed data. The background knowledge is expressed as a prior distribution and combined with observational data in the form of a likelihood function to determine the posterior distribution. The posterior can also be used for making predictions about future events. This Primer describes the stages involved in Bayesian analysis, from specifying the prior and data models to deriving inference, model checking and refinement. We discuss the importance of prior and posterior predictive checking, selecting a proper technique for sampling from a posterior distribution, variational inference and variable selection. Examples of successful applications of Bayesian analysis across various research fields are provided, including in social sciences, ecology, genetics, medicine and more. We propose strategies for reproducibility and reporting standards, outlining an updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian Statistics) checklist. Finally, we outline the impact of Bayesian analysis on artificial intelligence, a major goal in the next decade. ✉},
  file      = {:files/Schoot2021.pdf:pdf},
  isbn      = {0123456789},
  journal   = {Nature Reviews Methods Primers},
  keywords  = {Scientific community, Statistics},
  month     = {dec},
  publisher = {Springer Science and Business Media LLC},
  year      = {2021},
}

@Article{Wainer2007,
  author    = {Wainer, Howard},
  title     = {{The Most Dangerous Equation}},
  doi       = {10.1511/2007.65.249},
  issn      = {0003-0996},
  number    = {3},
  pages     = {249},
  url       = {http://www.americanscientist.org/issues/feature/the-most-dangerous-equation},
  volume    = {95},
  abstract  = {The urbanization that characterized the 20th century led to the abandonment of the rural lifestyle and, with it, an increase in the size of schools. The era of one-room schoolhouses was replaced by one with large schools—often with more than a thousand students, ... $\backslash$n},
  journal   = {American Scientist},
  publisher = {Sigma Xi},
  year      = {2007},
}

@Article{Ludlow2008,
  author    = {Ludlow, R. Frederick and Otto, Sijbren},
  title     = {{Systems chemistry}},
  doi       = {10.1039/B611921M},
  issn      = {0306-0012},
  number    = {1},
  pages     = {101--108},
  url       = {http://xlink.rsc.org/?DOI=B611921M},
  volume    = {37},
  abstract  = {The study of complex mixtures of interacting synthetic molecules has historically not received much attention from chemists, even though research into complexity is well established in the neighbouring fields. However, with the huge recent interest in systems biology and the availability of modern analytical techniques this situation is likely to change. In this tutorial review we discuss some of the incentives for developing systems chemistry and we highlight the pioneering work in which molecular networks are making a splash. A distinction is made between networks under thermodynamic and kinetic control. The former include dynamic combinatorial libraries while the latter involve pseudo-dynamic combinatorial libraries, oscillating reactions and networks of autocatalytic and replicating compounds. These studies provide fundamental insights into the organisational principles of molecular networks and how these give rise to emergent properties such as amplification and feedback loops, and may eventually shed light on the origin of life. The knowledge obtained from the study of molecular networks should ultimately enable us to engineer new systems with properties and functions unlike any conventional materials. {\textcopyright} The Royal Society of Chemistry.},
  file      = {:files/Ludlow2008.pdf:pdf},
  journal   = {Chem. Soc. Rev.},
  publisher = {Royal Society of Chemistry},
  year      = {2008},
}

@Article{Hoffman2011,
  author     = {Hoffman, Matthew D. and Gelman, Andrew},
  title      = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
  note       = {arXiv: 1111.4246},
  url        = {http://arxiv.org/abs/1111.4246},
  urldate    = {2021-12-13},
  abstract   = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient turnkey sampling algorithms.},
  annote     = {Comment: 30 pages, 7 figures},
  file       = {:Hoffman2011.pdf:PDF},
  journal    = {arXiv:1111.4246 [cs, stat]},
  keywords   = {Statistics - Computation, Computer Science - Machine Learning},
  month      = nov,
  shorttitle = {The {No}-{U}-{Turn} {Sampler}},
  year       = {2011},
}

@Article{McNeish2016,
  author    = {McNeish, Daniel},
  title     = {{On Using Bayesian Methods to Address Small Sample Problems}},
  doi       = {10.1080/10705511.2016.1186549},
  issn      = {1070-5511},
  number    = {5},
  pages     = {750--773},
  url       = {https://www.tandfonline.com/action/journalInformation?journalCode=hsem20 https://www.tandfonline.com/doi/full/10.1080/10705511.2016.1186549},
  volume    = {23},
  file      = {:files/McNeish2016.pdf:pdf},
  journal   = {Structural Equation Modeling: A Multidisciplinary Journal},
  month     = {sep},
  publisher = {Daniel McNeish},
  year      = {2016},
}

@Article{Semenov2015,
  author    = {Semenov, Sergey N. and Wong, Albert S.Y. and {Van Der Made}, R. Martijn and Postma, Sjoerd G.J. and Groen, Joost and {Van Roekel}, Hendrik W.H. and {De Greef}, Tom F.A. and Huck, Wilhelm T.S.},
  title     = {{Rational design of functional and tunable oscillating enzymatic networks}},
  doi       = {10.1038/nchem.2142},
  issn      = {1755-4349},
  number    = {2},
  pages     = {160--165},
  volume    = {7},
  abstract  = {Nature Chemistry 7, 160 (2015). doi:10.1038/nchem.2142},
  file      = {:files/Semenov2015.pdf:pdf},
  isbn      = {1755-4330$\backslash$r1755-4349},
  journal   = {Nature Chemistry},
  month     = {jan},
  pmid      = {25615670},
  publisher = {Nature Publishing Group},
  year      = {2015},
}

@Article{Roekel2015,
  author    = {van Roekel, Hendrik W. H. and Rosier, Bas J H M and Meijer, Lenny H H and Hilbers, Peter A J and Markvoort, Albert J. and Huck, Wilhelm T S and de Greef, Tom F. A.},
  title     = {{Programmable chemical reaction networks: emulating regulatory functions in living cells using a bottom-up approach}},
  doi       = {10.1039/C5CS00361J},
  issn      = {0306-0012},
  number    = {21},
  pages     = {7465--7483},
  url       = {http://xlink.rsc.org/?DOI=C5CS00361J},
  volume    = {44},
  abstract  = {Living cells are able to produce a wide variety of biological responses when subjected to biochemical stimuli. It has become apparent that these biological responses are regulated by complex chemical reaction networks (CRNs). Unravelling the function of these circuits is a key topic of both systems biology and synthetic biology. Recent progress at the interface of chemistry and biology together with the realisation that current experimental tools are insufficient to quantitatively understand the molecular logic of pathways inside living cells has triggered renewed interest in the bottom-up development of CRNs. This builds upon earlier work of physical chemists who extensively studied inorganic CRNs and showed how a system of chemical reactions can give rise to complex spatiotemporal responses such as oscillations and pattern formation. Using purified biochemical components, in vitro synthetic biologists have started to engineer simplified model systems with the goal of mimicking biological responses of intracellular circuits. Emulation and reconstruction of system-level properties of intracellular networks using simplified circuits are able to reveal key design principles and molecular programs that underlie the biological function of interest. In this Tutorial Review, we present an accessible overview of this emerging field starting with key studies on inorganic CRNs followed by a discussion of recent work involving purified biochemical components. Finally, we review recent work showing the versatility of programmable biochemical reaction networks (BRNs) in analytical and diagnostic applications.},
  file      = {:files/Roekel2015.pdf:pdf;:files/2015_van Roekel et al._Programmable chemical reaction networks emulating regulatory functions in living cells using a bottom-up approach.pdf:pdf},
  journal   = {Chemical Society Reviews},
  month     = {oct},
  publisher = {The Royal Society of Chemistry},
  year      = {2015},
}

@Book{Efron2016,
  author    = {Efron, Bradley and Hastie, Trevor},
  title     = {{Computer Age Statistical Inference}},
  isbn      = {9781107149892},
  pages     = {496},
  publisher = {Cambridge University Pr.},
  year      = {2016},
}

@Article{Boccaletti2006,
  author   = {Boccaletti, Stefano And Latora, V. And Moreno, Y. And Chavez, M. And Hwang, D. U.},
  title    = {{Complex networks: Structure and dynamics}},
  doi      = {10.1016/j.physrep.2005.10.009},
  issn     = {0370-1573},
  number   = {4-5},
  pages    = {175--308},
  url      = {www.elsevier.com/locate/physrep https://linkinghub.elsevier.com/retrieve/pii/S037015730500462X BOCCALETTI2006 - Complex Networks_ Structure and Dynamics.pdf},
  volume   = {424},
  abstract = {Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks' dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines, ranging from nonlinear science to biology, from statistical mechanics to medicine and engineering. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
  file     = {:files/Boccaletti2006.pdf:pdf},
  journal  = {Physics Reports},
  month    = {feb},
  year     = {2006},
}

@Article{Novak2008,
  author   = {Novák, Béla and Tyson, John J},
  title    = {{Design principles of biochemical oscillators}},
  doi      = {10.1038/nrm2530},
  issn     = {1471-0072},
  number   = {12},
  pages    = {981--991},
  url      = {www.nature.com/reviews/molcellbio http://www.nature.com/articles/nrm2530},
  volume   = {9},
  abstract = {Biochemical oscillations occur in many contexts (such as metabolism, signalling and development) and control important aspects of cell physiology, such as circadian rhythms, DNA synthesis, mitosis and the development of somites in vertebrate embryos (TABLE 1). In the 1950s and 1960s, the first clear examples of biochemical oscillations (in metabolic systems) were recognized in glycolysis 1,2 , cyclic AMP production 3 and the horseradish peroxidase reaction 4,5. Soon after these discoveries were made, theo-reticians were thinking about the general requirements for chemical oscillations and the specific mechanisms of these examples 6,7. After the molecular biology revolution of the 1980s, many new examples of oscillations in protein interaction networks (PINs) and in gene-regulatory networks (GRNs) came to light, such as the period (PER) proteins in animal circadian control 8 , the cyclin proteins in eukaryotic cell-cycle control 9,10 and the repressilator 11 in genetically engineered bacteria. Understanding the molecular basis of cellular oscillations is more than an exercise in experimental genetics and biochemistry. Oscillators have systems-level characteristics (for example, periodicity, robustness and entrainment) that transcend the properties of individual molecules or reaction partners and that involve the full topology of the reaction network. These properties can only be fully understood by viewing experimental data from a theoretical perspective and by quantitative mathematical modelling of chemical oscillatory processes. These models address general concepts of dynamical systems, such as feedback, time delays, bistability and hysteresis. In this review, we present a series of examples of increasing complexity that illustrate the essential requirements for biochemical oscillators. First, negative feedback is necessary to carry a reaction network back to the 'starting point' of its oscillation. Second, the negative-feedback signal must be sufficiently delayed in time so that the chemical reactions do not settle on a stable steady state. Third, the kinetic rate laws of the reaction mechanism must be sufficiently 'nonlinear' to destabilize the steady state. Fourth, the reactions that produce and consume the interacting chemical species must occur on appropriate timescales that permit the network to generate oscillations. Time delay can be created by a physical constraint (for example, the minimal time necessary to carry out transcription and translation, or the time needed to transport chemical species between cellular compartments), by a long chain of reaction intermediates (as in a metabolic pathway) or by dynamical hysteresis (overshoot and undershoot, as consequences of positive feedback in the reaction mechanism). To keep the mathematical details of oscillating chemical reactions to a minimum, we will demonstrate the design principles of biochemical oscillators by rate plots (which show how reaction rates depend on chemical concentrations), signal-response curves (which show how oscillations turn on and off in response to regulatory signals) and 'constraint' diagrams (which show how the kinetic constants of a reaction mechanism are constrained by requirements for periodicity). The mathematics are available to interested readers in Supplementary information S1-S4 (boxes). For further details on the principles that underlie chemical and biochemical oscillations, we refer readers to books 12-15 and review articles 2,6,16-18. Somites Early segmentations of the body of a vertebrate embryo that are laid down in a temporally and spatially periodic pattern. Robustness The notion that a control system should function reliably in the face of expected perturbations from outside the control system and from inevitable internal fluctuations. Abstract | Cellular rhythms are generated by complex interactions among genes, proteins and metabolites. They are used to control every aspect of cell physiology, from signalling, motility and development to growth, division and death. We consider specific examples of oscillatory processes and discuss four general requirements for biochemical oscillations: negative feedback, time delay, sufficient 'nonlinearity' of the reaction kinetics and proper balancing of the timescales of opposing chemical reactions. Positive feedback is one mechanism to delay the negative-feedback signal. Biological oscillators can be classified according to the topology of the positive-and negative-feedback loops in the underlying regulatory mechanism.},
  file     = {:files/2008_Novák, Tyson_Design principles of biochemical oscillators.pdf:pdf},
  journal  = {Nature Reviews Molecular Cell Biology},
  month    = {dec},
  year     = {2008},
}

@Article{Liepe2014,
  author   = {Liepe, Juliane and Kirk, Paul and Filippi, Sarah and Toni, Tina and Barnes, Chris P. and Stumpf, Michael P H},
  title    = {{A framework for parameter estimation and model selection from experimental data in systems biology using approximate Bayesian computation}},
  doi      = {10.1038/nprot.2014.025},
  issn     = {1754-2189},
  number   = {2},
  pages    = {439--456},
  url      = {http://www.nature.com/articles/nprot.2014.025 https://europepmc.org/articles/PMC5081097 https://europepmc.org/article/pmc/pmc5081097},
  volume   = {9},
  abstract = {As modeling becomes a more widespread practice in the life sciences and biomedical sciences, researchers need reliable tools to calibrate models against ever more complex and detailed data. Here we present an approximate Bayesian computation (ABC) framework and software environment, ABC-SysBio, which is a Python package that runs on Linux and Mac OS X systems and that enables parameter estimation and model selection in the Bayesian formalism by using sequential Monte Carlo (SMC) approaches. We outline the underlying rationale, discuss the computational and practical issues and provide detailed guidance as to how the important tasks of parameter inference and model selection can be performed in practice. Unlike other available packages, ABC-SysBio is highly suited for investigating, in particular, the challenging problem of fitting stochastic models to data. In order to demonstrate the use of ABC-SysBio, in this protocol we postulate the existence of an imaginary reaction network composed of seven interrelated biological reactions (involving a specific mRNA, the protein it encodes and a post-translationally modified version of the protein), a network that is defined by two files containing 'observed' data that we provide as supplementary information. In the first part of the PROCEDURE, ABC-SysBio is used to infer the parameters of this system, whereas in the second part we use ABC-SysBio's relevant functionality to discriminate between two different reaction network models, one of them being the 'true' one. Although computationally expensive, the additional insights gained in the Bayesian formalism more than make up for this cost, especially in complex problems. {\textcopyright} 2014 Nature America, Inc. All rights reserved.},
  file     = {:files/Liepe2014.pdf:pdf;:files/2014_Liepe et al._A framework for parameter estimation and model selection from experimental data in systems biology using approximate B.pdf:pdf},
  journal  = {Nature Protocols},
  keywords = {ABC-SMC, Bayesian inference, Systems Biology, dynamical Systems, model selection, parameter inference, stochastic systems},
  month    = {feb},
  pmid     = {24457334},
  year     = {2014},
}

@Misc{Ashby2006,
  author    = {Ashby, Deborah},
  title     = {{Bayesian statistics in medicine: A 25 year review}},
  doi       = {10.1002/sim.2672},
  abstract  = {This review examines the state of Bayesian thinking as Statistics in Medicine was launched in 1982, reflecting particularly on its applicability and uses in medical research. It then looks at each subsequent five-year epoch, with a focus on papers appearing in Statistics in Medicine, putting these in the context of major developments in Bayesian thinking and computation with reference to important books, landmark meetings and seminal papers. It charts the growth of Bayesian statistics as it is applied to medicine and makes predictions for the future. From sparse beginnings, where Bayesian statistics was barely mentioned, Bayesian statistics has now permeated all the major areas of medical statistics, including clinical trials, epidemiology, meta-analyses and evidence synthesis, spatial modelling, longitudinal modelling, survival modelling, molecular genetics and decision-making in respect of new technologies. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
  booktitle = {Statistics in Medicine},
  issn      = {0277-6715},
  keywords  = {Bayesian, Clinical trials, Longitudinal modelling, Medical statistics, Review, Spatial modelling},
  month     = {nov},
  number    = {21},
  pages     = {3589--3631},
  pmid      = {16947924},
  volume    = {25},
  year      = {2006},
}

@Article{Choi2017,
  author   = {Choi, Boseung and Rempala, Grzegorz A and Kim, Jae Kyoung},
  title    = {{Beyond the Michaelis-Menten equation: Accurate and efficient estimation of enzyme kinetic parameters}},
  doi      = {10.1038/s41598-017-17072-z},
  issn     = {2045-2322},
  number   = {1},
  pages    = {17018},
  url      = {www.nature.com/scientificreports http://www.nature.com/articles/s41598-017-17072-z},
  volume   = {7},
  abstract = {Examining enzyme kinetics is critical for understanding cellular systems and for using enzymes in industry. The Michaelis-Menten equation has been widely used for over a century to estimate the enzyme kinetic parameters from reaction progress curves of substrates, which is known as the progress curve assay. However, this canonical approach works in limited conditions, such as when there is a large excess of substrate over enzyme. Even when this condition is satisfied, the identifiability of parameters is not always guaranteed, and often not verifiable in practice. To overcome such limitations of the canonical approach for the progress curve assay, here we propose a Bayesian approach based on an equation derived with the total quasi-steady-state approximation. In contrast to the canonical approach, estimates obtained with this proposed approach exhibit little bias for any combination of enzyme and substrate concentrations. Importantly, unlike the canonical approach, an optimal experiment to identify parameters with certainty can be easily designed without any prior information. Indeed, with this proposed design, the kinetic parameters of diverse enzymes with disparate catalytic efficiencies, such as chymotrypsin, fumarase, and urease, can be accurately and precisely estimated from a minimal amount of timecourse data. A publicly accessible computational package performing such accurate and efficient Bayesian inference for enzyme kinetics is provided. Because enzymes can modulate biochemical reaction rates by selectively catalyzing specific substrates 1 , they play fundamental roles in metabolism, signal transduction, and cell regulation, and their malfunction can cause serious diseases 2,3. Furthermore, enzymes have been used as extremely specific catalysts in diverse industrial fields such as drug development, biofuel production, and food processing 4. A canonical approach used to understand enzyme kinetics for a century has been based on the Michaelis-Menten equation (MM equation), which was developed by Michaelis and Menten 5 and then was more rigorously derived by Briggs and Haldane 6 using the standard quasi-steady-state approximation (sQSSA) 7. The equation describes the dependence of enzyme-catalyzed reaction rates on the concentration of substrate by using two parameters, the catalytic constant, k cat and the Michaelis-Menten constant, K M (see below for details). The k cat determines the maximum rate of the reaction at saturating substrate concentrations, V max = k cat E T , where E T is total enzyme concentration, and the K M is the substrate concentration at which the reaction rate is half of V max. There are two major assays to estimate k cat and K M from a measured accumulation of product over time (i.e. progress curve): the initial velocity assay (initial rate analysis) and the reaction progress curve assay (progress curve analysis) 8-12. For the initial velocity assay, initial rates of the reaction are measured for a range of substrate concentrations. Then, by using a linear transform of these data, such as Lineweaver-Burk plots, the two parameters can be easily estimated without use of any computational tools 8,9. Recent advances in computational tools have led to an alternative approach: the reaction progress curve assay. In this assay, the entire timecourse (i.e. progress curve) is fitted to the solution of a differential equation or integrated rate equation, and thus the data},
  file     = {:files/Choi2017.pdf:pdf},
  journal  = {Scientific Reports},
  month    = {dec},
  year     = {2017},
}
